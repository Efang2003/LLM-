# 【书生·浦语大模型实战营】笔记 & 作业
## 目录
### [第一课](#第一课)
* [书生·浦语大模型全链路开源体系](#书生·浦语大模型全链路开源体系 )
* [internLM2技术报告](#internLM2技术报告)
### [第二课](#第二课)
### [第三课](#第三课)  
### [第四课](#第四课) 
### [第五课](#第五课) 
### [第六课](#第六课) 



## 第一课
### 书生·浦语大模型全链路开源体系
#### 背景
`ChatGPT`和`GPT-4`等`大语言模型（LLM）`的发展引发了关于`通用人工智能（AGI）`的讨论，即从针对特定任务的专用模型，转变为一个模型应对多种任务与模态。
#### 数据
`2TB`多模态数据 
* 书生·万卷1.0
* 书生·万卷CC
   
[数据获取](https://opendatalab.org.cn/)

#### 预训练
`InternLM-Train`
* 高可扩展
* 开箱即用
* 极致性能优化
* 兼容主流

#### 微调
`XTuner`  ：适配多种生态，多种硬件
  
***支持全参数微调和LoRA低成本微调***
* **有监督微调**
  * 使用场景：让基座模型学习到一些新知识，如某个垂类领域知识
  * 训练数据：文章、书籍、代码等
* **增量续训**
  * 使用场景：让模型学会理解各种指令进行对话，或者注入少量领域知识训练数据
  * 训练数据：高质量的对话、问答数据

#### 部署
`LMDeploy`：提供大模型在GPU上部署的全流程解决方案

* 高效推理引擎
* 完备易用的工具链
* 支持交互式推理

#### 评估
[`OpenCompass`](https://opencompass.org.cn/)
* CompassRank（性能排行榜）
* CompassKit（全栈评估工具）
* CompassHub（高质量评估基准社区）

#### 应用
* 轻量级智能体框架 `Lagent`
* 多模态代理工具箱 `AgentLego`
  
支持多种智能体，支持代码解释器等多种工具




  

### internLM2技术报告
[`原文链接`](https://arxiv.org/pdf/2403.17297.pdf)  
#### 创新技术亮点
* *长上下文处理能力*
* *全面性能改进*
* *增强对话与创作体验*
* *扩展工具调用功能*
* *强大的推理与数据分析能力*
#### 基础设施与模型架构
`InternEro`训练框架：具有**长序列训练**、**容错**、和**交互式训练**的功能
  
`LLaMA`设计原则：集成并重新配置矩阵，提高分布式环境的灵活性与效率
  
![alt](INTERN基础设施.jpg)
#### 预训练 Pre-train
##### 文本数据  
存储格式：`jsonl`  

![alt](数据处理.jpg)  

##### 代码数据      
存储格式：`markdown`    


##### 长文本数据    
数据过滤管道：长度选择、统计过滤器、语言模型perplexity  
#### Alignment
* ***有监督微调***
* ***COOL RLFH***
  * 整合多个偏好建立奖励机制
  * 将不同的prompt应用于不同类型的偏好
#### 评估与总结
`InternLM2`大语言模型在主观和客观的评估中都表现出出色的性能。  



## 第二课 笔记&作业
`项目实战课`
* 部署`InternLM2-Chat-1.8B`模型进行智能对话
  
结果： 
  
![alt](images/7.png)
* 部署实战营优秀作品`八戒-Chat-1.8B`模型

  
结果：

![alt](images/8.png)
* 通过`InternLM2-Chat-7B`运行 ***Lagent*** 智能体 Demo

结果：  

![alt](images/3.png)
* 实践部署`浦语·灵笔2`模型

结果：  

![alt](images/4.png)  
  
![alt](images/5.png)


## 第三课—— *RAG智能助手：以茴香豆为例*
### RAG介绍  

![alt](images/11.png)   

#### 技术概览
* 定义: ***RAG***是一种结合了检索和生成的技术，旨在通过利用**外部知识库**来增强大语言模型的性能。它通过检索与用户输入相关的信息片段生成更准确的答案
* 应用
  * 问答系统
  * 文本生成
  * 信息检索
  * 图片描述

  
***向量数据库（vector-DB）***：
* 数据存储：将文本等数据通过预训练模型转为固定长度的向量表示
* 相似性检索：根据用户的查询向量，使用向量数据库找出最相关向量的过程。通常通过计算*余弦相似度*或其他相似性度量来完成
* 向量表示的优化：使用更高级的文本编码技术，如句子嵌入或段落嵌入
#### 工作原理&流程 

![alt](images/9.png)  
![alt](images/RAG工作流程.png)  
 
#### 发展进程
* `Naive RAG`
  * 问答系统
  * 信息检索
* `Advanced RAG`
  * 摘要生成
  * 内容推荐
* `Modular RAG`
  * 多模态任务
  * 对话系统

#### 常见优化方法
* 嵌入优化（Embedding Optimization）
  * 结合稀疏和密集检索
  * 多任务
* 索引优化（Indexing Optimization）
  * 细粒度分割（Chunk）
  * 元数据
* 查询优化（Query Optimization）
  * 查询扩展、转换
  * 多查询
* 上下文管理（Context Optimization）
  * 重排（remark）
  * 上下文选择/压缩
* 迭代检索（Iterative Optimization）
  * 根据初始查询和迄今为止生成的文本进行重复搜索
* 递归检索（Recursive Optimization）
  * 迭代细化搜索查询
  * 链式推理（Chain-of-Thought）指导检索过程
* 自适应检索（Adaptive Optimization）
  * Flare，Self-RAG
  * 使用LLMs主动决定检索的最佳时机
#### RAG VS 微调（Fine-tuning） 
![alt](images/RAG和微调对比.png)  

#### 评估框架和基准测试
* 经典评估指标：`准确率` `召回率` `F1分数` `BLEU分数` `ROUGE分数`
* RAG评测框架
   * 基准测试 - RGB RECALL CRUD
   * 评测工具 - RAGAS ARES Trulens
### [`茴香豆`](https://github.com/InternLM/HuixiangDou)
#### 介绍
茴香豆是一个基于LLMs的领域知识助手，由书生浦语团队开发的开元大模型应用
* 专为即时通讯工具中的群聊场景优化的工作流，提供及时准确的技术支持和自动化问答服务
* 通过应用RAG技术，茴香豆能够理解和高效准确的回应与特定知识领域相关的复杂查询
#### 核心特征
* 开源免费
* 高效准确 Hybrid LLMs 专为群聊优化
* 领域知识
* 部署成本低
* 安全 可完全本地部署
* 扩展性强 兼容多种IM软件，支持多种开源LLMs和云端api
#### 工作流
* 多来源检索
  * 向量数据库
  * 网络搜索结果
  * 知识图谱
* 混合大模型
   * 本地LLM
   * 远程LLM
* 多重评分 拒答工作流
  * 回答有效
  * 避免信息泛滥
* 安全检查
  * 多种手段
  * 确保回答合规

## 第四课—— *XTuner微调小助手*
### 微调理论
#### 两种Finetune范式
* 增量预训练
  * 使用场景：让模型学到新知识，如某个垂直领域的常识
  * 训练数据：文章、书籍、代码等
* 指令跟随微调
   * 使用场景：让模型学到对话模版，根据人类指令进行对话
   * 训练数据：高质量的对话、问答数据

  
`对话模版`是为了让LLM区分出**System**、**User**和**Assistant**
#### LoRA & QLoRA
`LoRA`通过在原本的Linear旁，新增一个支路`Adapter`，包含两个连续的小Linear，参数量远小于之前的Linear，能大幅降低训练的显存消耗  
对比：  
![alt](images/对比.jpg)   

### XTuner介绍
#### 简介
* 搭配多种生态
  * 多种微调算法
  * 搭配多种开源生态
  * 自动优化加速
* 适配多种硬件
#### 特点
* 以*配置文件*的形式封装了大部分微调场景
* *轻量级*
#### 优化技巧
* `Flash Attention`  
将Attention计算并进行优化，避免Score NxN的显存占用
* `DeepSpeed ZeRO`  
通过将训练过程中的参数、梯度和优化器状态切片保存，能够在多GPU训练时显著节省显存
### 多模态LLM
#### 简介  
![alt](images/简介.jpg)  
#### LLaVA方案
给LLM增加视觉能力的过程，即是训练`Image projector`文件的过程；  
该过程分为两个阶段：`Pretrain`和`Finetune`
* *Pretrain*阶段示意图

![alt](images/P.jpg) 
* *Finetune*阶段示意图

![alt](images/F.jpg) 


## 第五课——`LMDeploy`量化部署LLM
### 大模型部署
#### 定义
* 在软件工程领域：将开发完毕的软件投入使用
* 在人工智能领域：实现深度学习算法落地应用的关键步骤
#### 场景
* **服务器端**：CPU部署等
* **边缘端/移动端**：移动机器人、手机等
#### 面临的挑战
* 计算量巨大（参数量巨大，前向推理时需要进行大量计算）
* 内存开销巨大
* 访存瓶颈
  目前硬件计算速度“远快于”显存带宽
* 动态请求
  * 请求量不确定
  * 请求时间不确定
  * Token逐个生成，生成数量不确定
#### 部署优化方法
* **模型剪枝**
  移除模型中不必要或多余的组件，比如参数，使模型更加高效
  * 非结构化剪枝：移除个别参数，而不考虑整体网络结构
  * 结构化剪枝：根据预定义规则移除连接或分层结构，同时保留整体网状结构
* **知识蒸馏**
  通过引导轻量化的学生模型“模仿”性能更好、结构更复杂的教师模型，不改变其结构而提高其性能
  * 上下文学习
  * 思维链
  * 指令跟随
* **量化**
  将传统方法中的浮点数转化为整数或其他离散形式，以减轻深度学习模型的存储和计算负担
  * 量化感知训练：使LLM适应低精度表示
  * 量化感知微调：确保LLM量化后仍保持其性能
  * 训练后量化：无需对LLM架构进行修改或重新训练
### LMDeploy
#### 简介
`LMDeploy`是涵盖了LLM任务的全套轻量化、部署和服务解决方案
* 高效的推理
* 可靠的量化
* 便捷的服务：通过*请求多发服务*，支持在多卡、多模型上的服务
* 有状态推理：通过缓存多轮对话中的k/v，记住对话历史
#### 核心功能
* 模型高效推理
  `TurboMind`高效推理引擎
* 模型量化压缩
  `W4A16量化（AWQ）`
* 服务化部署
  将LLM封装为HTTP API服务
#### 性能表现
卓越的推理能力  
![alt](images/LM性能表现.jpg) 


## 第六课——`Lagent`&`AgentLego`智能体应用搭建
### 智能体介绍
#### 起因——大语言模型具有局限性
* 幻觉
  模型可能会生成虚假信息，与现实世界严重不符或脱轨
* 时效性
  训练数据过时，无法反映最新趋势
* 可靠性
  面对复杂任务，可能频发错误输出的现象
#### 智能体概念
* 可以感知环境中的动态条件
* 能采取动作影响环境
* 能运用推理能力理解信息、解决问题、产生推断、决定动作
#### 智能体范式
* `AutoGPT`

![alt](images/Auto.jpg) 
* `ReWoo`

![alt](images/ReWoo.jpg) 
* `ReAct`

![alt](images/ReAct.jpg) 
### `Lagent`&`AgentLego`
* ***Lagent***
  轻量级开源智能体框架，旨在让用户可以高效地构建基于大语言模型的智能体
  * 支持多种智能体格式
  * 支持多种工具（如谷歌搜索、Python解释器等）
* ***AgentLego***
  多模态工具包，可以快速简便地扩展自定义工具，从而组装出自己的智能体
  * 支持多种智能体框架
  * 提供大量视觉、多模态领域前沿算法
* **两者关系**

![alt](images/两者关系.png) 












